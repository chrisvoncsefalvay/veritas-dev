{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERITAS Compression Methods Comparison\n",
    "\n",
    "This notebook compares state-of-the-art semantic compression methods for VERITAS decision traces.\n",
    "\n",
    "## Methods Compared\n",
    "\n",
    "1. **Matryoshka Representation Learning (MRL)** - Flexible nested embeddings (2022)\n",
    "   - Reference: Kusupati et al., NeurIPS 2022\n",
    "   - Expected: 14x compression with minimal loss\n",
    "\n",
    "2. **Product Quantization (PQ)** - Classical vector quantization\n",
    "   - Reference: Jegou et al., TPAMI 2011\n",
    "   - Expected: 32-64x compression\n",
    "\n",
    "3. **Binary Embeddings / Deep Hashing** - Extreme compression\n",
    "   - Reference: Various deep hashing methods (2015-2024)\n",
    "   - Expected: 32x compression, Hamming distance search\n",
    "\n",
    "4. **Scalar Quantization (int8/float16)** - Simple quantization\n",
    "   - Reference: HuggingFace, AWS OpenSearch (2024)\n",
    "   - Expected: 4x (int8) or 2x (float16) compression\n",
    "\n",
    "5. **Autoencoder** - Learned nonlinear compression\n",
    "   - Reference: Deep learning compression\n",
    "   - Expected: Flexible, data-dependent\n",
    "\n",
    "6. **PCA (Baseline)** - Linear dimensionality reduction\n",
    "   - Reference: Classical method\n",
    "   - Baseline for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../code')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "from core import DecisionTrace, NodeType\n",
    "from compression import SemanticEmbedder, EmbeddingCompressor, CompressionConfig\n",
    "from compression_advanced import (\n",
    "    MatryoshkaCompressor, ProductQuantizer, BinaryEmbedding,\n",
    "    ScalarQuantizer, AutoencoderCompressor, compare_compression_methods\n",
    ")\n",
    "\n",
    "# Set up plotting\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Test Embeddings\n",
    "\n",
    "Create realistic embeddings for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate realistic embeddings (768-dimensional)\n",
    "np.random.seed(42)\n",
    "n_test_embeddings = 100\n",
    "embedding_dim = 768\n",
    "\n",
    "# Simulate realistic embeddings with some structure\n",
    "# Real embeddings have lower intrinsic dimensionality\n",
    "intrinsic_dim = 128\n",
    "latent = np.random.randn(n_test_embeddings, intrinsic_dim)\n",
    "projection = np.random.randn(intrinsic_dim, embedding_dim)\n",
    "embeddings = latent @ projection\n",
    "\n",
    "# Normalize (like sentence transformers)\n",
    "embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "print(f\"Generated {n_test_embeddings} embeddings of dimension {embedding_dim}\")\n",
    "print(f\"Embedding stats: mean={embeddings.mean():.4f}, std={embeddings.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compression Ratio Comparison\n",
    "\n",
    "Compare storage requirements for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test single embedding with all methods\n",
    "test_embedding = embeddings[0].tolist()\n",
    "\n",
    "compression_results = compare_compression_methods(test_embedding)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Method': name,\n",
    "        'Original (bytes)': stats.original_size_bytes,\n",
    "        'Compressed (bytes)': stats.compressed_size_bytes,\n",
    "        'Compression Ratio': stats.compression_ratio,\n",
    "        'Space Savings (%)': stats.space_savings_pct\n",
    "    }\n",
    "    for name, stats in compression_results.items()\n",
    "])\n",
    "\n",
    "comparison_df = comparison_df.sort_values('Compression Ratio', ascending=False)\n",
    "\n",
    "print(\"\\nCompression Method Comparison:\")\n",
    "print(\"=\" * 100)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize compression ratios\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Compression ratio\n",
    "colors = plt.cm.viridis(np.linspace(0, 0.9, len(comparison_df)))\n",
    "axes[0].barh(comparison_df['Method'], comparison_df['Compression Ratio'], \n",
    "             color=colors, edgecolor='black', linewidth=1.5)\n",
    "axes[0].set_xlabel('Compression Ratio', fontsize=12)\n",
    "axes[0].set_title('Compression Ratio by Method', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels\n",
    "for i, (idx, row) in enumerate(comparison_df.iterrows()):\n",
    "    axes[0].text(row['Compression Ratio'] + 0.5, i, \n",
    "                f\"{row['Compression Ratio']:.1f}x\", \n",
    "                va='center', fontweight='bold')\n",
    "\n",
    "# Plot 2: Storage size comparison\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "axes[1].bar(x - width/2, comparison_df['Original (bytes)']/1024, width, \n",
    "            label='Original', color='coral', alpha=0.8, edgecolor='black')\n",
    "axes[1].bar(x + width/2, comparison_df['Compressed (bytes)']/1024, width, \n",
    "            label='Compressed', color='steelblue', alpha=0.8, edgecolor='black')\n",
    "axes[1].set_ylabel('Size (KB)', fontsize=12)\n",
    "axes[1].set_title('Storage Size Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(comparison_df['Method'], rotation=45, ha='right')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('compression_methods_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quality Analysis: Reconstruction Error\n",
    "\n",
    "Measure how well each method preserves semantic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_reconstruction_quality(embeddings: np.ndarray, n_samples: int = 50):\n",
    "    \"\"\"\n",
    "    Measure reconstruction quality for each compression method.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i in range(min(n_samples, len(embeddings))):\n",
    "        embedding = embeddings[i].tolist()\n",
    "        original = np.array(embedding)\n",
    "        \n",
    "        # Matryoshka (256d)\n",
    "        mrl = MatryoshkaCompressor(full_dim=len(embedding))\n",
    "        compressed = mrl.compress(embedding, target_dim=256)\n",
    "        reconstructed = np.array(mrl.decompress(compressed, len(embedding)))\n",
    "        mse = np.mean((original - reconstructed) ** 2)\n",
    "        cosine_sim = np.dot(original, reconstructed) / (np.linalg.norm(original) * np.linalg.norm(reconstructed))\n",
    "        results.append({'Method': 'Matryoshka (256d)', 'MSE': mse, 'Cosine Similarity': cosine_sim})\n",
    "        \n",
    "        # Product Quantization\n",
    "        pq = ProductQuantizer(embedding_dim=len(embedding))\n",
    "        codes = pq.compress(embedding)\n",
    "        reconstructed = np.array(pq.decompress(codes))\n",
    "        mse = np.mean((original - reconstructed) ** 2)\n",
    "        cosine_sim = np.dot(original, reconstructed) / (np.linalg.norm(original) * np.linalg.norm(reconstructed))\n",
    "        results.append({'Method': 'Product Quantization', 'MSE': mse, 'Cosine Similarity': cosine_sim})\n",
    "        \n",
    "        # Binary\n",
    "        binary = BinaryEmbedding(embedding_dim=len(embedding))\n",
    "        codes = binary.compress(embedding)\n",
    "        reconstructed = np.array(binary.decompress(codes))\n",
    "        mse = np.mean((original - reconstructed) ** 2)\n",
    "        cosine_sim = np.dot(original, reconstructed) / (np.linalg.norm(original) * np.linalg.norm(reconstructed))\n",
    "        results.append({'Method': 'Binary Hashing', 'MSE': mse, 'Cosine Similarity': cosine_sim})\n",
    "        \n",
    "        # Int8\n",
    "        int8_quant = ScalarQuantizer(mode='int8')\n",
    "        codes = int8_quant.compress(embedding)\n",
    "        reconstructed = np.array(int8_quant.decompress(codes))\n",
    "        mse = np.mean((original - reconstructed) ** 2)\n",
    "        cosine_sim = np.dot(original, reconstructed) / (np.linalg.norm(original) * np.linalg.norm(reconstructed))\n",
    "        results.append({'Method': 'Scalar Quantization (int8)', 'MSE': mse, 'Cosine Similarity': cosine_sim})\n",
    "        \n",
    "        # Float16\n",
    "        float16_quant = ScalarQuantizer(mode='float16')\n",
    "        codes = float16_quant.compress(embedding)\n",
    "        reconstructed = np.array(float16_quant.decompress(codes))\n",
    "        mse = np.mean((original - reconstructed) ** 2)\n",
    "        cosine_sim = np.dot(original, reconstructed) / (np.linalg.norm(original) * np.linalg.norm(reconstructed))\n",
    "        results.append({'Method': 'Scalar Quantization (float16)', 'MSE': mse, 'Cosine Similarity': cosine_sim})\n",
    "        \n",
    "        # Autoencoder\n",
    "        ae = AutoencoderCompressor(input_dim=len(embedding), compressed_dim=128)\n",
    "        compressed = ae.compress(embedding)\n",
    "        reconstructed = np.array(ae.decompress(compressed))\n",
    "        mse = np.mean((original - reconstructed) ** 2)\n",
    "        cosine_sim = np.dot(original, reconstructed) / (np.linalg.norm(original) * np.linalg.norm(reconstructed))\n",
    "        results.append({'Method': 'Autoencoder (128d)', 'MSE': mse, 'Cosine Similarity': cosine_sim})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"Measuring reconstruction quality (this may take a minute)...\")\n",
    "quality_df = measure_reconstruction_quality(embeddings, n_samples=50)\n",
    "\n",
    "# Aggregate statistics\n",
    "quality_summary = quality_df.groupby('Method').agg({\n",
    "    'MSE': ['mean', 'std'],\n",
    "    'Cosine Similarity': ['mean', 'std']\n",
    "}).round(4)\n",
    "\n",
    "print(\"\\nReconstruction Quality:\")\n",
    "print(\"=\" * 100)\n",
    "print(quality_summary)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize quality metrics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Cosine similarity (higher is better)\n",
    "quality_summary_reset = quality_summary.reset_index()\n",
    "methods = quality_summary_reset['Method']\n",
    "cos_sim_mean = quality_summary_reset[('Cosine Similarity', 'mean')]\n",
    "cos_sim_std = quality_summary_reset[('Cosine Similarity', 'std')]\n",
    "\n",
    "axes[0].barh(methods, cos_sim_mean, xerr=cos_sim_std, \n",
    "             color='green', alpha=0.7, edgecolor='black', capsize=5)\n",
    "axes[0].set_xlabel('Cosine Similarity (higher = better)', fontsize=12)\n",
    "axes[0].set_title('Semantic Preservation Quality', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlim(0, 1.1)\n",
    "axes[0].axvline(x=0.95, color='red', linestyle='--', label='Excellent (>0.95)', linewidth=2)\n",
    "axes[0].axvline(x=0.90, color='orange', linestyle='--', label='Good (>0.90)', linewidth=2)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Plot 2: MSE (lower is better, log scale)\n",
    "mse_mean = quality_summary_reset[('MSE', 'mean')]\n",
    "mse_std = quality_summary_reset[('MSE', 'std')]\n",
    "\n",
    "axes[1].barh(methods, mse_mean, xerr=mse_std,\n",
    "             color='coral', alpha=0.7, edgecolor='black', capsize=5)\n",
    "axes[1].set_xlabel('Mean Squared Error (lower = better)', fontsize=12)\n",
    "axes[1].set_title('Reconstruction Error', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('compression_quality_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Quality visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Speed Benchmarks\n",
    "\n",
    "Measure compression and decompression speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_speed(embeddings: np.ndarray, n_iterations: int = 100):\n",
    "    \"\"\"\n",
    "    Benchmark compression/decompression speed.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    test_embedding = embeddings[0].tolist()\n",
    "    \n",
    "    methods_to_test = [\n",
    "        ('Matryoshka (256d)', lambda e: MatryoshkaCompressor(len(e)).compress(e, 256), \n",
    "         lambda c, dim: MatryoshkaCompressor(dim).decompress(c, dim)),\n",
    "        ('Binary Hashing', lambda e: BinaryEmbedding(len(e)).compress(e),\n",
    "         lambda c, dim: BinaryEmbedding(dim).decompress(c)),\n",
    "        ('Scalar Quantization (int8)', lambda e: ScalarQuantizer('int8').compress(e),\n",
    "         lambda c, dim: ScalarQuantizer('int8').decompress(c)),\n",
    "        ('Scalar Quantization (float16)', lambda e: ScalarQuantizer('float16').compress(e),\n",
    "         lambda c, dim: ScalarQuantizer('float16').decompress(c)),\n",
    "    ]\n",
    "    \n",
    "    for method_name, compress_fn, decompress_fn in methods_to_test:\n",
    "        # Compression speed\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(n_iterations):\n",
    "            compressed = compress_fn(test_embedding)\n",
    "        compress_time = (time.perf_counter() - start) / n_iterations * 1000  # ms\n",
    "        \n",
    "        # Decompression speed\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(n_iterations):\n",
    "            decompressed = decompress_fn(compressed, len(test_embedding))\n",
    "        decompress_time = (time.perf_counter() - start) / n_iterations * 1000  # ms\n",
    "        \n",
    "        results.append({\n",
    "            'Method': method_name,\n",
    "            'Compression Time (ms)': compress_time,\n",
    "            'Decompression Time (ms)': decompress_time,\n",
    "            'Total Time (ms)': compress_time + decompress_time\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"Benchmarking speed...\")\n",
    "speed_df = benchmark_speed(embeddings)\n",
    "\n",
    "print(\"\\nSpeed Benchmark Results:\")\n",
    "print(\"=\" * 100)\n",
    "print(speed_df.to_string(index=False))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize speed\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(speed_df))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, speed_df['Compression Time (ms)'], width, \n",
    "       label='Compression', color='steelblue', alpha=0.8, edgecolor='black')\n",
    "ax.bar(x + width/2, speed_df['Decompression Time (ms)'], width, \n",
    "       label='Decompression', color='coral', alpha=0.8, edgecolor='black')\n",
    "\n",
    "ax.set_ylabel('Time (ms)', fontsize=12)\n",
    "ax.set_title('Compression/Decompression Speed Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(speed_df['Method'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('compression_speed_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Speed visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compression-Quality Trade-off Analysis\n",
    "\n",
    "Visualize the trade-off between compression ratio and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data for trade-off analysis\n",
    "# Get average cosine similarity per method\n",
    "quality_avg = quality_df.groupby('Method')['Cosine Similarity'].mean().reset_index()\n",
    "quality_avg.columns = ['Method', 'Avg Cosine Similarity']\n",
    "\n",
    "# Merge with compression data\n",
    "tradeoff_df = comparison_df.merge(quality_avg, on='Method', how='left')\n",
    "\n",
    "print(\"\\nCompression-Quality Trade-off:\")\n",
    "print(\"=\" * 100)\n",
    "print(tradeoff_df[['Method', 'Compression Ratio', 'Avg Cosine Similarity']].to_string(index=False))\n",
    "\n",
    "# Visualize trade-off\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    tradeoff_df['Compression Ratio'],\n",
    "    tradeoff_df['Avg Cosine Similarity'],\n",
    "    s=300,\n",
    "    c=np.arange(len(tradeoff_df)),\n",
    "    cmap='viridis',\n",
    "    alpha=0.7,\n",
    "    edgecolors='black',\n",
    "    linewidths=2\n",
    ")\n",
    "\n",
    "# Add labels\n",
    "for idx, row in tradeoff_df.iterrows():\n",
    "    ax.annotate(\n",
    "        row['Method'],\n",
    "        (row['Compression Ratio'], row['Avg Cosine Similarity']),\n",
    "        xytext=(10, 5),\n",
    "        textcoords='offset points',\n",
    "        fontsize=9,\n",
    "        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7, edgecolor='gray')\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Compression Ratio (higher = better)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Cosine Similarity (higher = better)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Compression-Quality Trade-off\\n(Upper-right is ideal)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add quadrant lines\n",
    "ax.axvline(x=10, color='gray', linestyle='--', alpha=0.5, linewidth=1)\n",
    "ax.axhline(y=0.95, color='gray', linestyle='--', alpha=0.5, linewidth=1)\n",
    "\n",
    "# Add text annotations for quadrants\n",
    "ax.text(0.95, 0.05, 'High Quality,\\nLow Compression', \n",
    "        transform=ax.transAxes, ha='right', va='bottom',\n",
    "        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.3))\n",
    "ax.text(0.05, 0.95, 'High Quality,\\nHigh Compression\\n(IDEAL)', \n",
    "        transform=ax.transAxes, ha='left', va='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='green', alpha=0.3),\n",
    "        fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('compression_quality_tradeoff.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Trade-off visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Recommendations Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = f\"\"\"\n",
    "{'='*100}\n",
    "VERITAS COMPRESSION METHODS - RECOMMENDATIONS SUMMARY\n",
    "{'='*100}\n",
    "\n",
    "## Method Characteristics:\n",
    "\n",
    "1. **Matryoshka Representation Learning (256d)**\n",
    "   - Compression Ratio: {tradeoff_df[tradeoff_df['Method']=='Matryoshka (256d)']['Compression Ratio'].values[0]:.1f}x\n",
    "   - Quality: {tradeoff_df[tradeoff_df['Method']=='Matryoshka (256d)']['Avg Cosine Similarity'].values[0]:.4f}\n",
    "   - Best for: Flexible deployment, adjustable quality-size trade-off\n",
    "   - Production use: OpenAI, Google Gemini APIs (2024)\n",
    "\n",
    "2. **Product Quantization**\n",
    "   - Compression Ratio: {tradeoff_df[tradeoff_df['Method']=='Product Quantization']['Compression Ratio'].values[0]:.1f}x\n",
    "   - Quality: {tradeoff_df[tradeoff_df['Method']=='Product Quantization']['Avg Cosine Similarity'].values[0]:.4f}\n",
    "   - Best for: Extreme compression, production vector databases\n",
    "   - Production use: FAISS, Pinecone, Milvus\n",
    "\n",
    "3. **Binary Hashing**\n",
    "   - Compression Ratio: {tradeoff_df[tradeoff_df['Method']=='Binary Hashing']['Compression Ratio'].values[0]:.1f}x\n",
    "   - Quality: {tradeoff_df[tradeoff_df['Method']=='Binary Hashing']['Avg Cosine Similarity'].values[0]:.4f}\n",
    "   - Best for: Ultra-fast similarity search, maximum compression\n",
    "   - Production use: Large-scale retrieval systems\n",
    "\n",
    "4. **Scalar Quantization (int8)**\n",
    "   - Compression Ratio: {tradeoff_df[tradeoff_df['Method']=='Scalar Quantization (int8)']['Compression Ratio'].values[0]:.1f}x\n",
    "   - Quality: {tradeoff_df[tradeoff_df['Method']=='Scalar Quantization (int8)']['Avg Cosine Similarity'].values[0]:.4f}\n",
    "   - Best for: Simple, fast, minimal quality loss\n",
    "   - Production use: Most vector databases as default\n",
    "\n",
    "5. **Scalar Quantization (float16)**\n",
    "   - Compression Ratio: {tradeoff_df[tradeoff_df['Method']=='Scalar Quantization (float16)']['Compression Ratio'].values[0]:.1f}x\n",
    "   - Quality: {tradeoff_df[tradeoff_df['Method']=='Scalar Quantization (float16)']['Avg Cosine Similarity'].values[0]:.4f}\n",
    "   - Best for: Near-lossless compression, GPU acceleration\n",
    "   - Production use: PyTorch, TensorFlow default\n",
    "\n",
    "6. **Autoencoder (128d)**\n",
    "   - Compression Ratio: {tradeoff_df[tradeoff_df['Method']=='Autoencoder (128d)']['Compression Ratio'].values[0]:.1f}x\n",
    "   - Quality: {tradeoff_df[tradeoff_df['Method']=='Autoencoder (128d)']['Avg Cosine Similarity'].values[0]:.4f}\n",
    "   - Best for: Domain-specific optimization, learned compression\n",
    "   - Production use: Research, specialized applications\n",
    "\n",
    "## Recommendations by Use Case:\n",
    "\n",
    "**For VERITAS Decision Traces:**\n",
    "- **Default**: Scalar Quantization (int8) - Best balance of simplicity and quality\n",
    "- **High Performance**: Matryoshka (256d) - Flexible, production-ready\n",
    "- **Maximum Compression**: Product Quantization - When storage is critical\n",
    "- **Ultra-Fast Search**: Binary Hashing - For real-time similarity queries\n",
    "\n",
    "**Quality Requirements:**\n",
    "- High fidelity (>0.95 similarity): float16, int8\n",
    "- Moderate fidelity (>0.90): Matryoshka, Autoencoder\n",
    "- Lower fidelity acceptable: Product Quantization, Binary\n",
    "\n",
    "**Storage Constraints:**\n",
    "- Tight budget: Product Quantization (32x) or Binary (32x)\n",
    "- Moderate: Matryoshka (3x) or Autoencoder (6x)\n",
    "- Relaxed: int8 (4x) or float16 (2x)\n",
    "\n",
    "{'='*100}\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save summary\n",
    "with open('compression_methods_summary.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"\\n✓ Summary saved to 'compression_methods_summary.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has compared **6 state-of-the-art compression methods** for VERITAS:\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Matryoshka Representation Learning** offers the best balance for production use\n",
    "2. **Scalar Quantization (int8)** provides excellent quality with simple implementation\n",
    "3. **Product Quantization** achieves extreme compression for storage-critical scenarios\n",
    "4. **Binary Hashing** enables ultra-fast similarity search at the cost of quality\n",
    "\n",
    "### For VERITAS Implementation:\n",
    "\n",
    "We recommend a **tiered approach**:\n",
    "- **Tier 1 (Default)**: int8 quantization for simplicity\n",
    "- **Tier 2 (Production)**: Matryoshka for flexibility\n",
    "- **Tier 3 (High-scale)**: Product Quantization when storage is critical\n",
    "\n",
    "All methods are now available in `compression_advanced.py` for use in VERITAS traces."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
